---
title: "scraping"
format: html
---

# Setup

```{r}
#| message: false
#| warning: false

library(httr2)
library(jsonlite)
library(tidyverse)
library(sf)
library(arrow)
library(furrr)
library(Rcpp)
```

# API Abfrage

Beispielabfrage der Rasterdaten für einen bestimmten Breich mit `curl`. Genau so sieht die Abfrage aus, die von der Website gesendet wird, mit einem Unterschied: Als Dateiformat wird `pbf` verwendet (`?f=pbf`).
```
curl 'https://buergerplattform-prod2.production.netzda-mig.org/server/rest/services/Hosted/festnetz_010/FeatureServer/0/query?f=geojson&geometry=%7B%22xmin%22%3A467183.1168788754%2C%22ymin%22%3A5515695.96105688%2C%22xmax%22%3A469629.1017840004%2C%22ymax%22%3A5518141.945962004%7D&maxRecordCountFactor=3&orderByFields=objectid&outFields=*&outSR=25832&quantizationParameters=%7B%22extent%22%3A%7B%22spatialReference%22%3A%7B%22wkid%22%3A25832%7D%2C%22xmin%22%3A467183.1168788754%2C%22ymin%22%3A5515695.96105688%2C%22xmax%22%3A469629.1017840004%2C%22ymax%22%3A5518141.945962004%7D%2C%22mode%22%3A%22view%22%2C%22originPosition%22%3A%22upperLeft%22%2C%22tolerance%22%3A4.77731426782227%7D&resultType=tile&returnCentroid=true&returnExceededLimitFeatures=false&spatialRel=esriSpatialRelIntersects&where=1%3D1&geometryType=esriGeometryEnvelope&inSR=25832' --compressed -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:131.0) Gecko/20100101 Firefox/131.0' -H 'Accept: */*' -H 'Accept-Language: de,en-US;q=0.7,en;q=0.3' -H 'Accept-Encoding: gzip, deflate, br, zstd' -H 'Referer: https://webclient.eks.production.netzda-mig.org/' -H 'Origin: https://webclient.eks.production.netzda-mig.org' -H 'DNT: 1' -H 'Connection: keep-alive' -H 'Sec-Fetch-Dest: empty' -H 'Sec-Fetch-Mode: cors' -H 'Sec-Fetch-Site: same-site' -H 'Priority: u=4' -H 'TE: trailers' --output testdata3.geojson
```

Um Daten für verschiedene Jahre zu erhalten, kann der `festnetz_XXX` Teil angepasst werden:
- `festnetz_001` für 12.2018
- `festnetz_002` für 06.2019
- `festnetz_003` für 12.2019
- ...
- `festnetz_010` für 12.2023

Im folgenden wird eine Funktion für die API abfrage definiert. Input ist eine Gitterzelle, bzw. die Koordinaten der Eckpunkt (`xmin`, `xmax`, `ymin`, `ymax`) als Liste. Die Funktion gibt dann ein Dataframe mit den Daten für diese Zelle für alle Jahre zurück. Alle Koordinaten sind im EPSG:3035 System.

Es könnte auch `outFields=*` angepasst werden, z.B. mit `outFields=down_fn_hh_ftthb_400, down_fn_hh_ftthb_1000` um nur diese beiden Variablen zu erhalten. Das könnte die Abfrage etwas beschleunigen. Insbesondere die geschäftlichen Anschlüsse (`down_fn_gew_`, `down_fn_gwg_`) interessieren mich nicht.

**Achtung:** Bis einschließlich 06.2021 sind die Daten auf einem 250m Raster! Das wird unten bereinigt.

```{r}
# Base request template
base_req <- request("https://buergerplattform-prod2.production.netzda-mig.org") %>%
  req_url_path_append("server/rest/services/Hosted") %>%
  req_headers(
    `User-Agent` = "Mozilla/5.0",
    `Accept` = "*/*",
    `Origin` = "https://webclient.eks.production.netzda-mig.org/",
    `Referer` = "https://webclient.eks.production.netzda-mig.org/"
  ) %>%
  req_url_query(
    f = "geojson",
    maxRecordCountFactor = 3,
    orderByFields = "objectid",
    outFields = "*",
    outSR = 3035,
    resultType = "tile",
    returnCentroid = "true",
    returnExceededLimitFeatures = "false",
    spatialRel = "esriSpatialRelIntersects",
    where = "1=1",
    geometryType = "esriGeometryEnvelope",
    inSR = 3035
  )

dates <- c(
  "011" = "202406",
  "010" = "202312",
  "009" = "202306",
  "008" = "202212",
  "007" = "202206",
  "006" = "202106",
  "005" = "202012",
  "004" = "202006",
  "003" = "201912",
  "002" = "201906",
  "001" = "201812"
)

fetch_api <- function(geometry) {
  # Create requests for all dates
  requests <- names(dates) %>%
    map(\(x) {
      base_req %>%
        req_url_path_append(paste0("festnetz_", x)) %>%
        req_url_path_append("FeatureServer/0/query") %>%
        req_url_query(geometry = toJSON(geometry, auto_unbox = TRUE))
    })

  # Perform requests in parallel
  responses <- req_perform_parallel(requests)

  # Process responses
  map2_dfr(responses, dates, \(resp, date) {
    content <- resp %>%
      resp_body_json(simplifyVector = TRUE)

    if (content$properties$exceededTransferLimit) {
      print("Transfer Limit Exceeded at\n")
      print(geometry)
      return(NULL)
    }
    if (length(content$features) == 0)
      return(NULL)

    data_geom <- content %>%
      pluck("features") %>%
      pluck("centroid") %>%
      unnest_wider(coordinates, names_sep = "_") %>%
      select(x_mp = coordinates_1, y_mp = coordinates_2)

    data_content <- content %>%
      pluck("features") %>%
      pluck("properties") %>%
      select(starts_with("down") & contains("_hh_")) %>%
      mutate(across(everything(), as.numeric))

    cbind(data_geom, data_content) %>%
      mutate(date = date, .before = 1) %>%
      as_tibble() %>%
      return()
  })
}
```

# Abfrage laufen lassen

Der folgende Code erstellt ein 80x80 Gitternetz über Deutschland. Anschließend wird für jede Zelle die oben definierte Funktion aufgerufen. Da die Ergebnisse sehr groß sind, wird zwischendurch abgespeichert.

**ACHTUNG:** Dieser Schritt dauert lange!

```{r}
# Create grid cells
deutschland <- giscoR::gisco_get_countries(country = "DE") %>% st_transform(3035)

deutschland_grid_cells <- deutschland %>%
  st_make_grid(n = c(96, 96), square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  filter(st_intersects(., deutschland, sparse = FALSE)[,1]) %>%
  st_geometry() %>%
  map(~ as.list(st_bbox(.)))

# Process in chunks
chunk_size <- 200
grid_chunks <- split(deutschland_grid_cells, ceiling(seq_along(deutschland_grid_cells)/chunk_size))

# Process each chunk
plan(multisession, workers = 4)
for(i in seq_along(grid_chunks)) {
  str_glue("{i} von {length(grid_chunks)}") %>% print()

  chunk_result <- future_map(grid_chunks[[i]], fetch_api, .progress = TRUE) %>% list_rbind()

  write_parquet(chunk_result, str_glue("./Daten/Breitbandatlas/Rohdaten/raster_chunk_{i}.parquet"))
  rm(chunk_result)
  gc()
}
plan(sequential)
```

# Bereinigen

Nun werden die Daten noch von Duplikaten bereinigt und als gruppierten Arrow-Dataset abgespeichert.
Ich benutze einen Foor-Loop, weil der Datensatz zu groß ist um komplett in RAM zu passen. Daher suche ich in jedem Jahr einzeln nach Duplikaten.

Für Zeitpunkte ab 2022 runde ich den Mittelpunkt jeder Zelle auf die nächste 50, um so die Zellen am 100m LAEA Grid auszurichten. Vor 2022 sind die Zellen jedoch 250m groß. Hier suche ich um den Mittelpunkt der Zelle mit einem "Radius" von 125m nach Koordinaten die auf 50 Enden. Diese teile ich dann den ursprünglichen Koordinaten zu. Dadurch wird jede 250m Zelle auf im Mittel $(250m/100m)^2 = 6,25$ 100m Zellen aufgeteilt. Ich sage "im Mittel", da die genaue Anzahl schwankt. Für die Implementierung verwende ich `Rcpp`.

Durch die Aufteilung von 250m auf 100m entstehen viele Zellen, die es in den 100m Daten nicht gibt. Beispielsweise in einem kleinen Dorf, bei dem alle Häuser in einer 100m sowie auch der 250m Zelle liegen. Dort würden dann ca. 6 Zellen statt einer entstehen. Aus diesem Grund führe ich einen Inner-Join mit den 202312 Daten durch. Dadurch lösche ich alle überflüssigen Zellen, die durch die Aufteilung entstehen.

```{r}
dataset_old <- list.files(path = "./Daten/Breitbandatlas/Rohdaten",
                           pattern = "raster_chunk_.*\\.parquet",
                           full.names = TRUE) %>%
    open_dataset()

arrow_colnames_sorted <- dataset_old %>%
  slice_sample(n = 1) %>%
  select(starts_with("down")) %>%
  collect() %>%
  colnames() %>%
  gtools::mixedsort()

# Jahre mit 100m Raster
for(d in dates %>% head(-6) %>% unname()) {
  print(d)

  dataset_old %>%
    filter(date == d) %>%
    rename(x_mp_raw = x_mp,
           y_mp_raw = y_mp) %>%
    mutate(x_mp = floor(x_mp_raw / 100) * 100 + 50,
           y_mp = floor(y_mp_raw / 100) * 100 + 50) %>%
    select(date, x_mp, y_mp, all_of(arrow_colnames_sorted)) %>%
    distinct() %>%
    group_by(date) %>%
    write_dataset("./Daten/Breitbandatlas/Raster_100m")

    gc()
}

# Jahre vor 2022 mit 250m Raster, das aufgeteilt werden muss
sourceCpp("./src/cpp/get_coords.cpp")

data_202312 <- open_dataset("./Daten/Breitbandatlas/Raster_100m/") %>%
  filter(date == 202312) %>%
  select(x_mp, y_mp) %>%
  distinct() %>%
  collect()

for(d in dates %>% tail(6) %>% unname()) {
  print(d)

  data <- dataset_old %>%
    filter(date == d) %>%
    rename(x_mp_raw = x_mp,
           y_mp_raw = y_mp) %>%
    distinct() %>%
    collect()

  new_coords_100m <- get_coords_rcpp(data$x_mp_raw, data$y_mp_raw)

  data %>%
    left_join(new_coords_100m, by = join_by(x_mp_raw, y_mp_raw)) %>%
    select(date, x_mp, y_mp, all_of(arrow_colnames_sorted)) %>%
    distinct() %>%
    inner_join(data_202312, by = join_by(x_mp, y_mp)) %>%
    distinct() %>%
    group_by(date) %>%
    write_dataset("./Daten/Breitbandatlas/Raster_100m")

  gc()
}
```

# Beispiel

Der Datensatz kann jetzt wie folgt verwendet werden (beachte, dass die `date` Variable jetzt numeric ist):

```{r}
breitband_100m_arrow <- open_dataset("./Daten/Breitbandatlas/Raster_100m/")

breitband_100m_arrow %>%
  group_by(date) %>%
  summarise(down_fn_hh_ftthb_1000 = sum(down_fn_hh_ftthb_1000, na.rm = TRUE)) %>%
  collect() %>%
  mutate(date = ym(date)) %>%
  ggplot(aes(x = date, y = down_fn_hh_ftthb_1000))+
  geom_line()

breitband_100m_arrow %>%
  group_by(date) %>%
  count() %>%
  arrange(date) %>%
  collect()
```

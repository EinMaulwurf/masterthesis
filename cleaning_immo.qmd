---
title: "Immo Daten aufbereiten"
format: html
---

# Setup

```{r}
#| message: false
#| warning: false

library(tidyverse)
library(vroom)
library(arrow)
```

# Import
Ich habe zuerst die verschiedenen .csv Dateien in eine Datei zusammengefasst:

```{bash}
#| eval: false

# Header von erster Datei in neue Datei `combined` schreiben
head -n 1 WMSUF1.csv > combined
for f in *.csv; do tail -n +2 "$f" >> combined; done
# Eventuell ist folgende Methode schneller:
# awk 'FNR>1' *.csv >> combined.csv
# Getestet habe ich es nicht.
```

# Kauf

Problem: In den Daten ab 2019 sind die `dupID_gen` komisch. Z.B. `Other missing,Other missing,Other missing,Other missing,Other missing,Other missing,Real-estate agent ,Other missing,1,8794233,"ID occurs once only, or first occurence of ID"` oder `Other missing,Other missing,Other missing,Other missing,Other missing,Other missing,Real-estate agent ,Other missing,2,8307328,Probably one spell`.

Vorest bereinige ich es daher nicht.

```{r}
vroom_col_kauf <- cols_only(
  obid = col_character(),
  gkz = col_character(),
  plz = col_character(),
  kaufpreis = col_double(),
  baujahr = col_integer(),
  wohnflaeche = col_double(),
  zimmeranzahl = col_double(),
  ajahr = col_integer(),
  amonat = col_integer(),
  jahr = col_integer(),
  emonat = col_integer(),
  r1_id = col_character(),
  dupID_gen = col_character()
)

good_dupIDs <- c("ID occurs once only, or first occurence of ID", "Large differences in important features", "Like (4) but time gape >6 months", "Like (1) but time gap >6 months")

import_vroom_kauf <- vroom("./Daten/Immo/Rohdaten/Kauf/combined", col_types = vroom_col_kauf)

import_vroom_kauf_clean <- import_vroom_kauf %>%
  filter(!r1_id %in% c("-11", "-9")) %>%
  #filter(dupID_gen %in% good_dupIDs) %>%
  select(-dupID_gen) %>%
  drop_na(kaufpreis) %>%
  filter(kaufpreis %>% between(., quantile(., 0.01), quantile(., 0.99))) %>%
  mutate(baujahr = na_if(baujahr, -1)) %>%
  rename(ejahr = jahr) %>%
  mutate(x_mp = r1_id %>% str_extract("^\\d{4}") %>% as.numeric(),
         y_mp = r1_id %>% str_extract("\\d{4}$") %>% as.numeric(),
         x_mp = x_mp * 1000 + 500,
         y_mp = y_mp * 1000 + 500)

import_vroom_kauf_clean %>%
  group_by(ejahr) %>%
  write_dataset("./Daten/Immo/Kauf/")
```

```{r}
open_dataset("./Daten/Immo/Kauf/") %>%
  group_by(ejahr) %>%
  count() %>%
  collect()
```


# Miete

Gleiches Problem mit `dupID_gen` wie oben. Vorest bereinige ich es daher nicht.

```{r}
vroom_col_miete <- cols_only(
  obid = col_character(),
  gkz = col_character(),
  plz = col_character(),
  mietekalt = col_double(),
  baujahr = col_integer(),
  wohnflaeche = col_double(),
  zimmeranzahl = col_double(),
  ajahr = col_integer(),
  amonat = col_integer(),
  jahr = col_integer(),
  emonat = col_integer(),
  r1_id = col_character(),
  dupID_gen = col_character()
)

good_dupIDs <- c("ID occurs once only, or first occurence of ID", "Large differences in important features", "Like (4) but time gape >6 months", "Like (1) but time gap >6 months")

import_vroom_miete <- vroom("./Daten/Immo/Rohdaten/Miete/combined", col_types = vroom_col_miete)

import_vroom_miete_clean <- import_vroom_miete %>%
  filter(!r1_id %in% c("-11", "-9")) %>%
  #filter(dupID_gen %in% good_dupIDs) %>%
  select(-dupID_gen) %>%
  drop_na(mietekalt) %>%
  drop_na(wohnflaeche) %>%
  filter(mietekalt %>% between(., quantile(., 0.01), quantile(., 0.99))) %>%
  filter(wohnflaeche %>% between(., quantile(., 0.01), quantile(., 0.99))) %>%
  mutate(baujahr = na_if(baujahr, -1)) %>%
  rename(ejahr = jahr) %>%
  mutate(x_mp = r1_id %>% str_extract("^\\d{4}") %>% as.numeric(),
         y_mp = r1_id %>% str_extract("\\d{4}$") %>% as.numeric(),
         x_mp = x_mp * 1000 + 500,
         y_mp = y_mp * 1000 + 500)

import_vroom_miete_clean %>%
  group_by(ejahr) %>%
  write_dataset("./Daten/Immo/Miete/")
```

Achtung: bei `import_vroom_miete_clean` sind nur Daten bis 2019. Irgendwie werden also spätere Anzeigen alle rausgefiltert. Möglicherweise wegen der `dupID_gen`.

```{r}
import_vroom_miete_clean_test <- import_vroom_miete %>%
  filter(!r1_id %in% c("-11", "-9")) %>%
  #filter(dupID_gen %in% good_dupIDs) %>%
  #select(-dupID_gen) %>%
  drop_na(mietekalt) %>%
  drop_na(wohnflaeche) %>%
  filter(mietekalt %>% between(., quantile(., 0.01), quantile(., 0.99))) %>%
  filter(wohnflaeche %>% between(., quantile(., 0.01), quantile(., 0.99))) %>%
  mutate(baujahr = na_if(baujahr, -1)) %>%
  rename(ejahr = jahr) %>%
  mutate(x_mp = r1_id %>% str_extract("^\\d{4}") %>% as.numeric(),
         y_mp = r1_id %>% str_extract("\\d{4}$") %>% as.numeric(),
         x_mp = x_mp * 1000 + 500,
         y_mp = y_mp * 1000 + 500)

import_vroom_miete_clean_test %>%
  filter(ejahr > 2019) %>%
  count(dupID_gen)
```

```{r}
open_dataset("./Daten/Immo/Miete/") %>%
  group_by(ejahr) %>%
  count() %>%
  collect()
```

# RWI GEO REDX PUF

HK = House sales, WK = Apartment sales, WM = Apartment rents, CI = Combined Index

```{r}
rwi_geo_redx_puf <- readxl::read_xlsx("./Daten/RWI_GEO_REDX_PUF/RWIGEOREDX_GRIDS_v13_PUF.xlsx",
                          sheet = "Grids_RegionEff_yearly") %>%
  pivot_longer(
    cols = starts_with(c("pindex", "NOBS")),
    names_to = c(".value", "year"),
    names_pattern = "(pindex|NOBS)(\\d{4})"
  ) %>%
  rename(nobs = NOBS) %>%
  mutate(x_mp = grid %>% str_extract("^\\d{4}") %>% as.numeric(),
         y_mp = grid %>% str_extract("\\d{4}$") %>% as.numeric(),
         x_mp = x_mp * 1000 + 500,
         y_mp = y_mp * 1000 + 500,
         .keep = "unused",
         .before = 1) %>%
  pivot_wider(names_from = housing_type, values_from = c(pindex, nobs)) %>%
  mutate(year = as.numeric(year))

rwi_geo_redx_puf %>%
  write_parquet("./Daten/RWI_GEO_REDX_PUF/rwi_geo_redx_puf.parquet")
```

```{r}
rwi_geo_redx_puf %>%
  inner_join(
    cells_gemeinden_bbsr %>%
      filter(gemeindetyp_differenziert_name == "Größere Großstadt")
  ) %>%
  filter(year >= 2018)
```


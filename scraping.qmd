---
title: "scraping"
format: html
---

# Setup

```{r}
#| message: false
#| warning: false

library(httr2)
library(jsonlite)
library(tidyverse)
library(sf)
library(arrow)
library(furrr)
```

# API Abfrage

Beispielabfrage der Rasterdaten für einen bestimmten Breich mit `curl`. Genau so sieht die Abfrage aus, die von der Website gesendet wird, mit einem Unterschied: Als Dateiformat wird `pbf` verwendet (`?f=pbf`).
```
curl 'https://buergerplattform-prod2.production.netzda-mig.org/server/rest/services/Hosted/festnetz_010/FeatureServer/0/query?f=geojson&geometry=%7B%22xmin%22%3A467183.1168788754%2C%22ymin%22%3A5515695.96105688%2C%22xmax%22%3A469629.1017840004%2C%22ymax%22%3A5518141.945962004%7D&maxRecordCountFactor=3&orderByFields=objectid&outFields=*&outSR=25832&quantizationParameters=%7B%22extent%22%3A%7B%22spatialReference%22%3A%7B%22wkid%22%3A25832%7D%2C%22xmin%22%3A467183.1168788754%2C%22ymin%22%3A5515695.96105688%2C%22xmax%22%3A469629.1017840004%2C%22ymax%22%3A5518141.945962004%7D%2C%22mode%22%3A%22view%22%2C%22originPosition%22%3A%22upperLeft%22%2C%22tolerance%22%3A4.77731426782227%7D&resultType=tile&returnCentroid=true&returnExceededLimitFeatures=false&spatialRel=esriSpatialRelIntersects&where=1%3D1&geometryType=esriGeometryEnvelope&inSR=25832' --compressed -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:131.0) Gecko/20100101 Firefox/131.0' -H 'Accept: */*' -H 'Accept-Language: de,en-US;q=0.7,en;q=0.3' -H 'Accept-Encoding: gzip, deflate, br, zstd' -H 'Referer: https://webclient.eks.production.netzda-mig.org/' -H 'Origin: https://webclient.eks.production.netzda-mig.org' -H 'DNT: 1' -H 'Connection: keep-alive' -H 'Sec-Fetch-Dest: empty' -H 'Sec-Fetch-Mode: cors' -H 'Sec-Fetch-Site: same-site' -H 'Priority: u=4' -H 'TE: trailers' --output testdata3.geojson
```

Um Daten für verschiedene Jahre zu erhalten, kann der `festnetz_XXX` Teil angepasst werden:
- `festnetz_001` für 12.2018
- `festnetz_002` für 07.2019
- `festnetz_003` für 12.2019
- ...
- `festnetz_010` für 12.2023

Im folgenden wird eine Funktion für die API abfrage definiert. Input ist eine Gitterzelle, bzw. die Koordinaten der Eckpunkt (xmin, xmax, ymin, ymax) als Liste. Die Funktion gibt dann ein Dataframe mit den Daten für diese Zelle für alle Jahre zurück. Alle Koordinaten sind im EPSG:3035 System.

```{r}
# Base request template
base_req <- request("https://buergerplattform-prod2.production.netzda-mig.org") %>%
  req_url_path_append("server/rest/services/Hosted") %>%
  req_headers(
    `User-Agent` = "Mozilla/5.0",
    `Accept` = "*/*",
    `Origin` = "https://webclient.eks.production.netzda-mig.org/",
    `Referer` = "https://webclient.eks.production.netzda-mig.org/"
  ) %>%
  req_url_query(
    f = "geojson",
    maxRecordCountFactor = 3,
    orderByFields = "objectid",
    outFields = "*",
    outSR = 3035,
    resultType = "tile",
    returnCentroid = "true",
    returnExceededLimitFeatures = "false",
    spatialRel = "esriSpatialRelIntersects",
    where = "1=1",
    geometryType = "esriGeometryEnvelope",
    inSR = 3035
  )

dates <- c(
  "010" = "202312",
  "009" = "202306",
  "008" = "202212",
  "007" = "202206",
  "006" = "202107",
  "005" = "202012",
  "004" = "202007",
  "003" = "201912",
  "002" = "201907",
  "001" = "201812"
)

fetch_api <- function(geometry) {
  # Create requests for all dates
  requests <- names(dates) %>%
    map(\(x) {
      base_req %>%
        req_url_path_append(paste0("festnetz_", x)) %>%
        req_url_path_append("FeatureServer/0/query") %>%
        req_url_query(geometry = toJSON(geometry, auto_unbox = TRUE))
    })
  
  # Perform requests in parallel
  responses <- req_perform_parallel(requests)
  
  # Process responses
  map2_dfr(responses, dates, \(resp, date) {
    content <- resp %>%
      resp_body_json(simplifyVector = TRUE)
    
    if (content$properties$exceededTransferLimit) {
      print("Transfer Limit Exceeded at\n")
      print(geometry)
      return(NULL)
    }
    if (length(content$features) == 0)
      return(NULL)
    
    data_geom <- content %>%
      pluck("features") %>%
      pluck("centroid") %>%
      unnest_wider(coordinates, names_sep = "_") %>%
      select(x_mp = coordinates_1, y_mp = coordinates_2)
    
    data_content <- content %>%
      pluck("features") %>%
      pluck("properties") %>%
      select(starts_with("down") & contains("_hh_")) %>%
      mutate(across(everything(), as.numeric))
    
    cbind(data_geom, data_content) %>%
      mutate(date = date, .before = 1) %>%
      as_tibble() %>%
      return()
  })
}
```

# Abfrage laufen lassen

Der folgende Code erstellt ein 80x80 Gitternetz über Deutschland. Anschließend wird für jede Zelle die oben definierte Funktion aufgerufen. Da die Ergebnisse sehr groß sind, wird zwischendurch abgespeichert.

**ACHTUNG:** Dieser Schritt dauert lange!

```{r}
# Create grid cells
deutschland <- giscoR::gisco_get_countries(country = "DE") %>% st_transform(3035)

deutschland_grid_cells <- deutschland %>%
  st_make_grid(n = c(96, 96), square = TRUE, what = "polygons") %>%
  st_as_sf() %>%
  filter(st_intersects(., deutschland, sparse = FALSE)[,1]) %>%
  st_geometry() %>%
  map(~ as.list(st_bbox(.)))

# Process in chunks
chunk_size <- 200
grid_chunks <- split(deutschland_grid_cells, ceiling(seq_along(deutschland_grid_cells)/chunk_size))

# Process each chunk
plan(multisession, workers = 4)
for(i in seq_along(grid_chunks)) {
  str_glue("{i} von {length(grid_chunks)}") %>% print()
  
  chunk_result <- future_map(grid_chunks[[i]], fetch_api, .progress = TRUE) %>% list_rbind()
  
  write_parquet(chunk_result, str_glue("./Daten/Breitbandatlas/raster_chunk_{i}.parquet"))
  rm(chunk_result)
  gc()
}
plan(sequential)
```

Nun werden die Daten noch von Duplikaten bereinigt und als gruppierten Arrow-Dataset abgespeichert.
Ich benutze einen Foor-Loop, weil der Datensatz zu groß ist um komplett in RAM zu passen. Daher suche ich in jedem Jahr einzeln nach Duplikaten. Den Mittelpunkt jeder Gitterzelle runde ich auf 50 um die Zellen am LAEA Grid auszurichten.

```{r}
dataset_old <- list.files(path = "./Daten/Breitbandatlas", 
                           pattern = "raster_chunk_.*\\.parquet", 
                           full.names = TRUE) %>%
    open_dataset()

for(d in unname(dates)) {
  print(d)
  
  dataset_old %>%
    filter(date == d) %>%
    mutate(x_mp = floor(x_mp / 100) * 100 + 50,
           y_mp = floor(y_mp / 100) * 100 + 50) %>%
    distinct() %>%
    group_by(date) %>%
    write_dataset("./Daten/Breitbandatlas/Raster")
    
    gc()
}
```

# Beispiel

Der Datensatz kann jetzt wie folgt verwendet werden (beachte, dass die `date` Variable jetzt numeric ist):

```{r}
final_result <- open_dataset("./Daten/Breitbandatlas/Raster/")

final_result %>%
  group_by(date) %>%
  summarise(down_fn_hh_ftthb_1000 = sum(down_fn_hh_ftthb_1000, na.rm = TRUE)) %>%
  collect() %>%
  mutate(date = ym(date)) %>%
  ggplot(aes(x = date, y = down_fn_hh_ftthb_1000))+
  geom_line()

final_result %>%
  count() %>%
  collect()
```

